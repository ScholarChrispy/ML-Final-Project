{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ML Final Project**:\n",
    "\n",
    "### **Group Members:**\n",
    "- Christopher Johnson (christopher.johnson13@ontariotechu.net)\n",
    "- Name (Student Email)\n",
    "- Name (Student Email)\n",
    "- Name (Student Email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Goals & Outline:**\n",
    "The goal of this project is to use sentiment analysis to analyze the content of tweets, and make decisions on whether their sentiment is positive, negative, or neutral.\n",
    "\n",
    "### Outline:\n",
    "1. Data Importing and Preprocessing\n",
    "2. Model Construction\n",
    "   1. RNN\n",
    "   2. LSTM\n",
    "   3. ???\n",
    "3. Model Training\n",
    "   1. RNN\n",
    "   2. LSTM\n",
    "   3. ???\n",
    "4. Model Analysis and Comparison\n",
    "5. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing Packages & Libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# general packages/libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime # used to convert Date_time strings to useable format\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "\n",
    "# torchmetrics\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "# torchtext\n",
    "import torchtext.data\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# lightning\n",
    "from lightning.pytorch import LightningModule\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "# tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# nltk\n",
    "import nltk # used for tokenziation\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# regular expressions\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Importing and Preprocessing:**\n",
    "This section is where the tweet data is imported and processed into tokens. This tokenization process is required so the neural network architectures can interpret the text data.\n",
    "\n",
    "**Tokenization definition:** the process of breaking down a sequence of information into smaller chunks known as tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>entity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Tweet content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2402</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>So I spent a few hours making something for fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2402</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>So I spent a couple of hours doing something f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2402</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>So I spent a few hours doing something for fun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2402</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>So I spent a few hours making something for fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2402</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2010 So I spent a few hours making something f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2402</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2403</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Rock-Hard La Varlope, RARE &amp; POWERFUL, HANDSOM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2403</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Rock-Hard La Varlope, RARE &amp; POWERFUL, HANDSOM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2403</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Rock-Hard La Varlope, RARE &amp; POWERFUL, HANDSOM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2403</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Rock-Hard La Vita, RARE BUT POWERFUL, HANDSOME...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2403</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Live Rock - Hard music La la Varlope, RARE &amp; t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2403</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>I-Hard like me, RARE LONDON DE, HANDSOME 2011,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2404</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>that was the first borderlands session in a lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2404</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>this was the first Borderlands session in a lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Tweet ID       entity sentiment  \\\n",
       "0       2401  Borderlands  Positive   \n",
       "1       2401  Borderlands  Positive   \n",
       "2       2401  Borderlands  Positive   \n",
       "3       2401  Borderlands  Positive   \n",
       "4       2401  Borderlands  Positive   \n",
       "5       2401  Borderlands  Positive   \n",
       "6       2402  Borderlands  Positive   \n",
       "7       2402  Borderlands  Positive   \n",
       "8       2402  Borderlands  Positive   \n",
       "9       2402  Borderlands  Positive   \n",
       "10      2402  Borderlands  Positive   \n",
       "11      2402  Borderlands  Positive   \n",
       "12      2403  Borderlands   Neutral   \n",
       "13      2403  Borderlands   Neutral   \n",
       "14      2403  Borderlands   Neutral   \n",
       "15      2403  Borderlands   Neutral   \n",
       "16      2403  Borderlands   Neutral   \n",
       "17      2403  Borderlands   Neutral   \n",
       "18      2404  Borderlands  Positive   \n",
       "19      2404  Borderlands  Positive   \n",
       "\n",
       "                                        Tweet content  \n",
       "0   im getting on borderlands and i will murder yo...  \n",
       "1   I am coming to the borders and I will kill you...  \n",
       "2   im getting on borderlands and i will kill you ...  \n",
       "3   im coming on borderlands and i will murder you...  \n",
       "4   im getting on borderlands 2 and i will murder ...  \n",
       "5   im getting into borderlands and i can murder y...  \n",
       "6   So I spent a few hours making something for fu...  \n",
       "7   So I spent a couple of hours doing something f...  \n",
       "8   So I spent a few hours doing something for fun...  \n",
       "9   So I spent a few hours making something for fu...  \n",
       "10  2010 So I spent a few hours making something f...  \n",
       "11                                                was  \n",
       "12  Rock-Hard La Varlope, RARE & POWERFUL, HANDSOM...  \n",
       "13  Rock-Hard La Varlope, RARE & POWERFUL, HANDSOM...  \n",
       "14  Rock-Hard La Varlope, RARE & POWERFUL, HANDSOM...  \n",
       "15  Rock-Hard La Vita, RARE BUT POWERFUL, HANDSOME...  \n",
       "16  Live Rock - Hard music La la Varlope, RARE & t...  \n",
       "17  I-Hard like me, RARE LONDON DE, HANDSOME 2011,...  \n",
       "18  that was the first borderlands session in a lo...  \n",
       "19  this was the first Borderlands session in a lo...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the tweet data from the CSV using pandas\n",
    "tweet_data = pd.read_csv('./Datasets/twitter_training.csv', names=['Tweet ID', 'entity', 'sentiment', 'Tweet content'])\n",
    "tweet_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "Negative    22542\n",
       "Positive    20832\n",
       "Neutral     18318\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# droppping \"irrelevant\" sentiment values\n",
    "tweet_data.drop(\n",
    "    tweet_data[tweet_data['sentiment'] == 'Irrelevant'].index,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# showing remaining sentiment distribution\n",
    "tweet_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     im getting on borderlands and i will murder yo...\n",
      "1     I am coming to the borders and I will kill you...\n",
      "2     im getting on borderlands and i will kill you ...\n",
      "3     im coming on borderlands and i will murder you...\n",
      "4     im getting on borderlands 2 and i will murder ...\n",
      "5     im getting into borderlands and i can murder y...\n",
      "6     So I spent a few hours making something for fu...\n",
      "7     So I spent a couple of hours doing something f...\n",
      "8     So I spent a few hours doing something for fun...\n",
      "9     So I spent a few hours making something for fu...\n",
      "10    2010 So I spent a few hours making something f...\n",
      "11                                                  was\n",
      "12    Rock-Hard La Varlope, RARE & POWERFUL, HANDSOM...\n",
      "13    Rock-Hard La Varlope, RARE & POWERFUL, HANDSOM...\n",
      "14    Rock-Hard La Varlope, RARE & POWERFUL, HANDSOM...\n",
      "15    Rock-Hard La Vita, RARE BUT POWERFUL, HANDSOME...\n",
      "16    Live Rock - Hard music La la Varlope, RARE & t...\n",
      "17    I-Hard like me, RARE LONDON DE, HANDSOME 2011,...\n",
      "18    that was the first borderlands session in a lo...\n",
      "19    this was the first Borderlands session in a lo...\n",
      "20    that was the first borderlands session in a lo...\n",
      "21    that was the first borderlands session in a lo...\n",
      "22    that I was the first real borderlands session ...\n",
      "23    that was the first borderlands session in a ho...\n",
      "24    the biggest dissappoinment in my life came out...\n",
      "25    The biggest disappointment of my life came a y...\n",
      "26    The biggest disappointment of my life came a y...\n",
      "27    the biggest dissappoinment in my life coming o...\n",
      "28    For the biggest male dissappoinment in my life...\n",
      "29    the biggest dissappoinment in my life came bac...\n",
      "30    WE FINISHED BORDERLANDS 3 FINALLY YAS! Thank y...\n",
      "31    WE FINALLY FINALLY FIND BORDERLANDS 3 YES! Tha...\n",
      "32    Thank you for hanging up everyone! It was fun....\n",
      "33    WE FINISHED BORDERLANDS 3 UPDATE YAS! Thank yo...\n",
      "34    WE FINISHED BORDERLANDS 3 AND FINALLY YAS! Tha...\n",
      "35    WE FINISHED BORDERLANDS 3 FINALLY YAS! Hey you...\n",
      "36    Man Gearbox really needs to fix this dissapoin...\n",
      "37    Man Gearbox really needs to fix these disappoi...\n",
      "38    Man Gearbox really needs to fix this disssapoi...\n",
      "39    Man Bethesda really needs to fix this dissapoi...\n",
      "40    Man Gearbox really needs to fix this dissapoin...\n",
      "41    <unk> Gearbox really time to fix this 10 drops...\n",
      "42                     Check out this epic streamer!.  \n",
      "43                       Check out this epic streamer!.\n",
      "44                         Watch this epic striptease!.\n",
      "45                        Check out our epic streamer!.\n",
      "46                   Check out this big epic streamer!.\n",
      "47                      Check<unk> this epic streamer!.\n",
      "48    Blaming Sight for Tardiness! A little bit of b...\n",
      "49    A bit of borderland. I was called to work tomo...\n",
      "50    Guilty of sobriety! A bit of a borderline. I w...\n",
      "51    Blaming Sight for Tardiness! A little bit of b...\n",
      "52    for Blaming Sight for Tardiness! A little bit ...\n",
      "53                                                  all\n",
      "54    why does like every man in borderlands have sl...\n",
      "55    Why, like every man in border countries, have ...\n",
      "56    Why, like everyone else in the border countrie...\n",
      "57    why does like<unk> man in borderlands have sli...\n",
      "58    why Beth does like every man in borderlands ha...\n",
      "59    why does practically every man in France have ...\n",
      "Name: Tweet content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# extracting the the tweet content for ease of use\n",
    "content = tweet_data['Tweet content']\n",
    "\n",
    "# showing the extracted content\n",
    "print(content.head(60))\n",
    "\n",
    "# convert content to an iterator\n",
    "# content = iter(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "method that processes content and removes the following content:\n",
    "- 1 word tweets\n",
    "- tweets that contain only numbers\n",
    "- tweets that contain only special characters (e.g. /, ., <, etc.)\n",
    "'''\n",
    "def process_content(values:pd.Series):\n",
    "    # convert series to list\n",
    "    values = values.to_list()\n",
    "\n",
    "    # records the number of tweets removed\n",
    "    # (used to adjust the access index)\n",
    "    num_removed = 0\n",
    "\n",
    "    for i in tqdm(range(0, len(values))):\n",
    "        # checks for 1 word tweet using a tokenizer\n",
    "        if (len(word_tokenize(str(values[i - num_removed]))) <= 1):\n",
    "            del values[i - num_removed]     # remove the tweet from the list\n",
    "            num_removed += 1     # count the number of removed tweets\n",
    "\n",
    "        # checks for tweets with only \"...\", \" \", \"[\" or, \"]\"\n",
    "        if (len(re.search(r'^(\\Q.\\E| |\\Q[\\E|\\Q]\\E|[0-9])*', values[i - num_removed])) > 0):\n",
    "            print(i)\n",
    "    return pd.Series(values)\n",
    "\n",
    "# method that removes links from tweets\n",
    "def remove_links(content:pd.Series):\n",
    "    return 0 # ! TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0bd99bff514c7db6c089b08c1d5513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "error",
     "evalue": "bad escape \\Q at position 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chris\\OneDrive\\Code\\OntarioTechCode\\(4) Year Four\\Semester One\\Machine Learning\\Final Project\\ML-Final-Project\\Project.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/chris/OneDrive/Code/OntarioTechCode/%284%29%20Year%20Four/Semester%20One/Machine%20Learning/Final%20Project/ML-Final-Project/Project.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m content \u001b[39m=\u001b[39m process_content(content)\n",
      "\u001b[1;32mc:\\Users\\chris\\OneDrive\\Code\\OntarioTechCode\\(4) Year Four\\Semester One\\Machine Learning\\Final Project\\ML-Final-Project\\Project.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/OneDrive/Code/OntarioTechCode/%284%29%20Year%20Four/Semester%20One/Machine%20Learning/Final%20Project/ML-Final-Project/Project.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         num_removed \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m     \u001b[39m# count the number of removed tweets\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/OneDrive/Code/OntarioTechCode/%284%29%20Year%20Four/Semester%20One/Machine%20Learning/Final%20Project/ML-Final-Project/Project.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# checks for tweets with only \"...\", \" \", \"[\" or, \"]\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chris/OneDrive/Code/OntarioTechCode/%284%29%20Year%20Four/Semester%20One/Machine%20Learning/Final%20Project/ML-Final-Project/Project.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mif\u001b[39;00m (\u001b[39mlen\u001b[39m(re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m^(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mQ.\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mE| |\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mQ[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mE|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mQ]\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mE|[0-9])*\u001b[39m\u001b[39m'\u001b[39m, values[i \u001b[39m-\u001b[39m num_removed])) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/OneDrive/Code/OntarioTechCode/%284%29%20Year%20Four/Semester%20One/Machine%20Learning/Final%20Project/ML-Final-Project/Project.ipynb#X30sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39mprint\u001b[39m(i)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/OneDrive/Code/OntarioTechCode/%284%29%20Year%20Four/Semester%20One/Machine%20Learning/Final%20Project/ML-Final-Project/Project.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mSeries(values)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\Lib\\re\\__init__.py:176\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearch\u001b[39m(pattern, string, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m    174\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[39m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39msearch(string)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\Lib\\re\\__init__.py:294\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe re.TEMPLATE/re.T flag is deprecated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mas it is an undocumented flag \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mwithout an obvious purpose. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mDon\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt use it.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    293\u001b[0m               \u001b[39mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m--> 294\u001b[0m p \u001b[39m=\u001b[39m _compiler\u001b[39m.\u001b[39mcompile(pattern, flags)\n\u001b[0;32m    295\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (flags \u001b[39m&\u001b[39m DEBUG):\n\u001b[0;32m    296\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(_cache) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m _MAXCACHE:\n\u001b[0;32m    297\u001b[0m         \u001b[39m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\Lib\\re\\_compiler.py:743\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[39mif\u001b[39;00m isstring(p):\n\u001b[0;32m    742\u001b[0m     pattern \u001b[39m=\u001b[39m p\n\u001b[1;32m--> 743\u001b[0m     p \u001b[39m=\u001b[39m _parser\u001b[39m.\u001b[39mparse(p, flags)\n\u001b[0;32m    744\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    745\u001b[0m     pattern \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\Lib\\re\\_parser.py:982\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(str, flags, state)\u001b[0m\n\u001b[0;32m    979\u001b[0m state\u001b[39m.\u001b[39mflags \u001b[39m=\u001b[39m flags\n\u001b[0;32m    980\u001b[0m state\u001b[39m.\u001b[39mstr \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\n\u001b[1;32m--> 982\u001b[0m p \u001b[39m=\u001b[39m _parse_sub(source, state, flags \u001b[39m&\u001b[39m SRE_FLAG_VERBOSE, \u001b[39m0\u001b[39m)\n\u001b[0;32m    983\u001b[0m p\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mflags \u001b[39m=\u001b[39m fix_flags(\u001b[39mstr\u001b[39m, p\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mflags)\n\u001b[0;32m    985\u001b[0m \u001b[39mif\u001b[39;00m source\u001b[39m.\u001b[39mnext \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\Lib\\re\\_parser.py:457\u001b[0m, in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    455\u001b[0m start \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39mtell()\n\u001b[0;32m    456\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 457\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m    458\u001b[0m                        \u001b[39mnot\u001b[39;00m nested \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m items))\n\u001b[0;32m    459\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sourcematch(\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    460\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\Lib\\re\\_parser.py:865\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    862\u001b[0m     group \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    863\u001b[0m sub_verbose \u001b[39m=\u001b[39m ((verbose \u001b[39mor\u001b[39;00m (add_flags \u001b[39m&\u001b[39m SRE_FLAG_VERBOSE)) \u001b[39mand\u001b[39;00m\n\u001b[0;32m    864\u001b[0m                \u001b[39mnot\u001b[39;00m (del_flags \u001b[39m&\u001b[39m SRE_FLAG_VERBOSE))\n\u001b[1;32m--> 865\u001b[0m p \u001b[39m=\u001b[39m _parse_sub(source, state, sub_verbose, nested \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m    866\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m source\u001b[39m.\u001b[39mmatch(\u001b[39m\"\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    867\u001b[0m     \u001b[39mraise\u001b[39;00m source\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mmissing ), unterminated subpattern\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    868\u001b[0m                        source\u001b[39m.\u001b[39mtell() \u001b[39m-\u001b[39m start)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\Lib\\re\\_parser.py:457\u001b[0m, in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    455\u001b[0m start \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39mtell()\n\u001b[0;32m    456\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 457\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m    458\u001b[0m                        \u001b[39mnot\u001b[39;00m nested \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m items))\n\u001b[0;32m    459\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sourcematch(\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    460\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\Lib\\re\\_parser.py:541\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[39mif\u001b[39;00m this[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 541\u001b[0m     code \u001b[39m=\u001b[39m _escape(source, this, state)\n\u001b[0;32m    542\u001b[0m     subpatternappend(code)\n\u001b[0;32m    544\u001b[0m \u001b[39melif\u001b[39;00m this \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m SPECIAL_CHARS:\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\Lib\\re\\_parser.py:440\u001b[0m, in \u001b[0;36m_escape\u001b[1;34m(source, escape, state)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(escape) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    439\u001b[0m         \u001b[39mif\u001b[39;00m c \u001b[39min\u001b[39;00m ASCIILETTERS:\n\u001b[1;32m--> 440\u001b[0m             \u001b[39mraise\u001b[39;00m source\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mbad escape \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m escape, \u001b[39mlen\u001b[39m(escape))\n\u001b[0;32m    441\u001b[0m         \u001b[39mreturn\u001b[39;00m LITERAL, \u001b[39mord\u001b[39m(escape[\u001b[39m1\u001b[39m])\n\u001b[0;32m    442\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n",
      "\u001b[1;31merror\u001b[0m: bad escape \\Q at position 2"
     ]
    }
   ],
   "source": [
    "content = process_content(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40    <unk> Gearbox really time to fix this 10 drops...\n",
       "41                     Check out this epic streamer!.  \n",
       "42                       Check out this epic streamer!.\n",
       "43                         Watch this epic striptease!.\n",
       "44                        Check out our epic streamer!.\n",
       "45                   Check out this big epic streamer!.\n",
       "46                      Check<unk> this epic streamer!.\n",
       "47    Blaming Sight for Tardiness! A little bit of b...\n",
       "48    A bit of borderland. I was called to work tomo...\n",
       "49    Guilty of sobriety! A bit of a borderline. I w...\n",
       "50    Blaming Sight for Tardiness! A little bit of b...\n",
       "51    for Blaming Sight for Tardiness! A little bit ...\n",
       "52    why does like every man in borderlands have sl...\n",
       "53    Why, like every man in border countries, have ...\n",
       "54    Why, like everyone else in the border countrie...\n",
       "55    why does like<unk> man in borderlands have sli...\n",
       "56    why Beth does like every man in borderlands ha...\n",
       "57    why does practically every man in France have ...\n",
       "58                                              . . [  \n",
       "59                                                 .. [\n",
       "60                                                .. 45\n",
       "61                                                 .. [\n",
       "62                                              .. what\n",
       "63    Going to finish up Borderlands 2 today. I've g...\n",
       "64    I will finish Borderlands 2 today. I have some...\n",
       "65    I'm going to finish Borderlands 2 today. I hav...\n",
       "66    Going to finish up Borderlands 2 today. I've g...\n",
       "67    Going to finish finish cleaning up Borderlands...\n",
       "68    Going to finish up volume 2 today. I've got so...\n",
       "69    imma probably play some borderlands tps in a b...\n",
       "70    imma is probably playing a bit of borderland t...\n",
       "71    imma will probably play some border tps in a b...\n",
       "72    imma probably play live borderlands tps in a b...\n",
       "73    for imma probably play some borderlands tps in...\n",
       "74    imma probably got some video tps in a bit. tha...\n",
       "75    One of our own @ProfZeroo is live w/ @borderla...\n",
       "76    One of our own @ ProfZeroo is live w / @ borde...\n",
       "77    One of our own @ ProfZero - live w / @ borderl...\n",
       "78    One of our own @ProfZeroo is live >/ @borderla...\n",
       "79    One of our shows own @ProfZeroo is live via w ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[40:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    2\n",
      "13    2\n",
      "14    2\n",
      "15    2\n",
      "16    2\n",
      "17    2\n",
      "18    0\n",
      "19    0\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# extracting the the sentiment data for ease of use\n",
    "sentiment = tweet_data['sentiment']\n",
    "\n",
    "# the numerical representations of the sentiment values\n",
    "sentiment_numerical = {\n",
    "    'Positive': 0,\n",
    "    'Negative': 1,\n",
    "    'Neutral': 2,\n",
    "}\n",
    "\n",
    "# converting the sentiment data into a numerical form\n",
    "sentiment.replace(to_replace=sentiment_numerical, inplace=True)\n",
    "\n",
    "# showing the converted sentiment data\n",
    "print(sentiment.head(20))\n",
    "\n",
    "# converting the sentiment data into a torch tensor\n",
    "sentiment = torch.tensor(sentiment, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803ffe3826e94f5d91f6693164c8e063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens = word_tokenize(content[0])\n",
    "print(type(tokens))\n",
    "\n",
    "def iterate_tokens(df):\n",
    "    for val in tqdm(df):\n",
    "        yield word_tokenize(str(val))\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    iterate_tokens(content),\n",
    "    min_freq = 5,\n",
    "    specials = ['<unk>']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Creation:**\n",
    "This is where the models being used are defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RNN Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
